{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b7a5c2-9377-4e75-ad3b-5d057f5c16df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "2pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5577890-7b5d-4302-a5f5-74605c6d03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: torch==2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.0.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66219f84-12fb-4783-a235-7f9c32f84d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: torchaudio in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "085cc184-b9a6-4196-954e-893af3f2a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.11-py3-none-any.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.4 kB ? eta -:--:--\n",
      "     -------- ------------------------------- 10.2/48.4 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 41.0/48.4 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 48.4/48.4 kB 349.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (0.15.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\cl502_25\\appdata\\roaming\\python\\python39\\site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\cl502_25\\appdata\\roaming\\python\\python39\\site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.3 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 14.8 MB/s eta 0:00:00\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-1.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec74725-695c-441d-ab35-29ed492c7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from timm import create_model\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59af437c-cce0-4f35-a1d5-b462c9abf8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 12556\n",
      "Validation samples: 3140\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data_path = '/Users/anishvirkhare/Downloads/archive/train' \n",
    "train_data = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optional: Check dataset sizes\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f692b8c8-5d52-48e0-b4d0-7d3350d2c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vision Transformer model with 23 classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model('vit_base_patch16_224', pretrained=True, num_classes=23).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211080a7-8001-4b9d-a0a4-71df9a7f2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "981c05c0-0f56-4be6-9103-51431a5cdb0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 1/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 Summary:\n",
      "  Training Loss: 2.8796, Training Accuracy: 15.40%\n",
      "  Validation Loss: 2.7370, Validation Accuracy: 18.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 2/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 Summary:\n",
      "  Training Loss: 2.6959, Training Accuracy: 20.04%\n",
      "  Validation Loss: 2.6950, Validation Accuracy: 21.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 3/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 Summary:\n",
      "  Training Loss: 2.5975, Training Accuracy: 22.98%\n",
      "  Validation Loss: 2.6079, Validation Accuracy: 22.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 4/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 Summary:\n",
      "  Training Loss: 2.5055, Training Accuracy: 25.06%\n",
      "  Validation Loss: 2.5564, Validation Accuracy: 23.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 5/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 Summary:\n",
      "  Training Loss: 2.4264, Training Accuracy: 27.23%\n",
      "  Validation Loss: 2.5410, Validation Accuracy: 23.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 6/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 Summary:\n",
      "  Training Loss: 2.3484, Training Accuracy: 29.77%\n",
      "  Validation Loss: 2.4762, Validation Accuracy: 26.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 7/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 Summary:\n",
      "  Training Loss: 2.2653, Training Accuracy: 31.33%\n",
      "  Validation Loss: 2.4802, Validation Accuracy: 27.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 8/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 Summary:\n",
      "  Training Loss: 2.1863, Training Accuracy: 33.46%\n",
      "  Validation Loss: 2.4454, Validation Accuracy: 27.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Training: 100%|█████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 9/30 - Validation: 100%|█████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 Summary:\n",
      "  Training Loss: 2.0774, Training Accuracy: 36.60%\n",
      "  Validation Loss: 2.4388, Validation Accuracy: 29.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 10/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:52<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 Summary:\n",
      "  Training Loss: 1.9628, Training Accuracy: 39.97%\n",
      "  Validation Loss: 2.3629, Validation Accuracy: 30.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 11/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 Summary:\n",
      "  Training Loss: 1.8605, Training Accuracy: 42.98%\n",
      "  Validation Loss: 2.4023, Validation Accuracy: 30.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 12/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 Summary:\n",
      "  Training Loss: 1.7113, Training Accuracy: 47.38%\n",
      "  Validation Loss: 2.3991, Validation Accuracy: 31.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:38<00:00,  1.16it/s]\n",
      "Epoch 13/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:52<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 Summary:\n",
      "  Training Loss: 1.5638, Training Accuracy: 51.54%\n",
      "  Validation Loss: 2.5251, Validation Accuracy: 29.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:39<00:00,  1.16it/s]\n",
      "Epoch 14/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:52<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 Summary:\n",
      "  Training Loss: 1.4165, Training Accuracy: 55.64%\n",
      "  Validation Loss: 2.5237, Validation Accuracy: 32.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:40<00:00,  1.15it/s]\n",
      "Epoch 15/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 Summary:\n",
      "  Training Loss: 1.2498, Training Accuracy: 60.83%\n",
      "  Validation Loss: 2.5813, Validation Accuracy: 33.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:26<00:00,  1.20it/s]\n",
      "Epoch 16/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 Summary:\n",
      "  Training Loss: 1.0895, Training Accuracy: 65.60%\n",
      "  Validation Loss: 2.6732, Validation Accuracy: 34.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:23<00:00,  1.22it/s]\n",
      "Epoch 17/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 Summary:\n",
      "  Training Loss: 0.9441, Training Accuracy: 69.35%\n",
      "  Validation Loss: 2.8150, Validation Accuracy: 34.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 18/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 Summary:\n",
      "  Training Loss: 0.7709, Training Accuracy: 75.29%\n",
      "  Validation Loss: 3.0242, Validation Accuracy: 32.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 19/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 Summary:\n",
      "  Training Loss: 0.6734, Training Accuracy: 78.12%\n",
      "  Validation Loss: 3.1500, Validation Accuracy: 33.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 20/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 Summary:\n",
      "  Training Loss: 0.5421, Training Accuracy: 82.20%\n",
      "  Validation Loss: 3.0658, Validation Accuracy: 34.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 21/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 Summary:\n",
      "  Training Loss: 0.4950, Training Accuracy: 83.55%\n",
      "  Validation Loss: 3.2910, Validation Accuracy: 36.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 22/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 Summary:\n",
      "  Training Loss: 0.4392, Training Accuracy: 85.46%\n",
      "  Validation Loss: 3.1997, Validation Accuracy: 33.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 23/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 Summary:\n",
      "  Training Loss: 0.3623, Training Accuracy: 88.02%\n",
      "  Validation Loss: 3.5891, Validation Accuracy: 34.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:21<00:00,  1.22it/s]\n",
      "Epoch 24/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 Summary:\n",
      "  Training Loss: 0.3453, Training Accuracy: 88.79%\n",
      "  Validation Loss: 3.3987, Validation Accuracy: 34.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 25/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 Summary:\n",
      "  Training Loss: 0.3215, Training Accuracy: 89.18%\n",
      "  Validation Loss: 3.7015, Validation Accuracy: 32.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:23<00:00,  1.22it/s]\n",
      "Epoch 26/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 Summary:\n",
      "  Training Loss: 0.3068, Training Accuracy: 90.00%\n",
      "  Validation Loss: 3.4196, Validation Accuracy: 35.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 27/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 Summary:\n",
      "  Training Loss: 0.2807, Training Accuracy: 90.50%\n",
      "  Validation Loss: 3.5776, Validation Accuracy: 32.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:22<00:00,  1.22it/s]\n",
      "Epoch 28/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 Summary:\n",
      "  Training Loss: 0.2851, Training Accuracy: 90.58%\n",
      "  Validation Loss: 3.5632, Validation Accuracy: 34.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:24<00:00,  1.21it/s]\n",
      "Epoch 29/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 Summary:\n",
      "  Training Loss: 0.2528, Training Accuracy: 91.69%\n",
      "  Validation Loss: 3.8626, Validation Accuracy: 35.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Training: 100%|████████████████████████████████████████████████████████| 393/393 [05:23<00:00,  1.22it/s]\n",
      "Epoch 30/30 - Validation: 100%|████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 Summary:\n",
      "  Training Loss: 0.9619, Training Accuracy: 73.58%\n",
      "  Validation Loss: 3.3663, Validation Accuracy: 9.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 30    # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Track validation accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "387c0ad4-8ad8-480d-8574-d21775f08907",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"C:\\\\Users\\\\cl502_25\\\\Downloads\\\\vit.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87177df0-5443-41ca-a2fe-5237197d1a43",
   "metadata": {},
   "source": [
    "### ViT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "391aa3cc-34fc-4cf7-8c2c-3d37bf986742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 4002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/lhc7fyqd4x17cs8p6p_jsyrh0000gn/T/ipykernel_1796/1949572461.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model_path = '/Users/anishvirkhare/Downloads/vit.pth'\n",
    "\n",
    "# Load the complete model\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define the same transformations used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load the test data\n",
    "test_data_path = \"/Users/anishvirkhare/Downloads/archive/test\"  # Replace with your test data path\n",
    "test_data = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "# Create DataLoader for the test data\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optional: Check test dataset size\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502c72ad-ff26-489b-9762-4b9eb6f60b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████| 126/126 [05:07<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "  Test Loss: 3.3508\n",
      "  Test Accuracy: 8.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "# Use the same loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Summary:\")\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7accb4d-ca22-4268-b1dc-257544025a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75199213-c0fe-4e70-ad89-fa4c1a611475",
   "metadata": {},
   "source": [
    "# 8 Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62de3e46-6f0c-4ef2-9c4b-6043a0f9d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd62975-1082-48b9-a3f8-6e43b7846aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in each class:\n",
      "Urticaria Hives: 212\n",
      "Seborrheic Keratoses and other Benign Tumors: 1371\n",
      "Poison Ivy Photos and other Contact Dermatitis: 260\n",
      "Acne and Rosacea Photos: 840\n",
      "Vascular Tumors: 482\n",
      "Eczema Photos: 1235\n",
      "Psoriasis pictures Lichen Planus and related diseases: 1405\n",
      "Exanthems and Drug Eruptions: 404\n",
      "Lupus and other Connective Tissue diseases: 420\n",
      "Scabies Lyme Disease and other Infestations and Bites: 431\n",
      "Bullous Disease Photos: 448\n",
      "Nail Fungus and other Nail Disease: 1040\n",
      "Tinea Ringworm Candidiasis and other Fungal Infections: 1300\n",
      "Systemic Disease: 606\n",
      "Light Diseases and Disorders of Pigmentation: 568\n",
      "Atopic Dermatitis Photos: 489\n",
      "Warts Molluscum and other Viral Infections: 1086\n",
      "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions: 1149\n",
      "Melanoma Skin Cancer Nevi and Moles: 463\n",
      "Vasculitis Photos: 416\n",
      "Cellulitis Impetigo and other Bacterial Infections: 288\n",
      "Hair Loss Photos Alopecia and other Hair Diseases: 239\n",
      "Herpes HPV and other STDs Photos: 405\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Path to the directory containing the training data\n",
    "data_dir = \"/Users/anishvirkhare/Downloads/archive/train\"\n",
    "\n",
    "# Count images in each class\n",
    "class_counts = {}\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        class_counts[class_name] = len([\n",
    "            file for file in os.listdir(class_path)\n",
    "            if os.path.isfile(os.path.join(class_path, file))\n",
    "        ])\n",
    "\n",
    "# Display the number of images in each class\n",
    "print(\"Number of images in each class:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"{class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38013e6-4424-4028-a9f1-dcc84af85ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Urticaria Hives' removed with only 212 images.\n",
      "Class 'Seborrheic Keratoses and other Benign Tumors' retained with 1371 images.\n",
      "Class 'Poison Ivy Photos and other Contact Dermatitis' removed with only 260 images.\n",
      "Class 'Acne and Rosacea Photos' retained with 840 images.\n",
      "Class 'Vascular Tumors' removed with only 482 images.\n",
      "Class 'Eczema Photos' retained with 1235 images.\n",
      "Class 'Psoriasis pictures Lichen Planus and related diseases' retained with 1405 images.\n",
      "Class 'Exanthems and Drug Eruptions' removed with only 404 images.\n",
      "Class 'Lupus and other Connective Tissue diseases' removed with only 420 images.\n",
      "Class 'Scabies Lyme Disease and other Infestations and Bites' removed with only 431 images.\n",
      "Class 'Bullous Disease Photos' removed with only 448 images.\n",
      "Class 'Nail Fungus and other Nail Disease' retained with 1040 images.\n",
      "Class 'Tinea Ringworm Candidiasis and other Fungal Infections' retained with 1300 images.\n",
      "Class 'Systemic Disease' removed with only 606 images.\n",
      "Class 'Light Diseases and Disorders of Pigmentation' removed with only 568 images.\n",
      "Class 'Atopic Dermatitis Photos' removed with only 489 images.\n",
      "Class 'Warts Molluscum and other Viral Infections' retained with 1086 images.\n",
      "Class 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions' retained with 1149 images.\n",
      "Class 'Melanoma Skin Cancer Nevi and Moles' removed with only 463 images.\n",
      "Class 'Vasculitis Photos' removed with only 416 images.\n",
      "Class 'Cellulitis Impetigo and other Bacterial Infections' removed with only 288 images.\n",
      "Class 'Hair Loss Photos Alopecia and other Hair Diseases' removed with only 239 images.\n",
      "Class 'Herpes HPV and other STDs Photos' removed with only 405 images.\n",
      "Filtered training data saved to: /Users/anishvirkhare/Downloads/8_classes_train\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Input parameters\n",
    "data_dir = \"/Users/anishvirkhare/Downloads/archive/train\"  # Path to the original training data\n",
    "output_dir = \"/Users/anishvirkhare/Downloads/8_classes_train\"  # Path to save the filtered training data\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        image_files = [\n",
    "            file for file in os.listdir(class_path)\n",
    "            if os.path.isfile(os.path.join(class_path, file))\n",
    "        ]\n",
    "        if len(image_files) >= 800:\n",
    "            # Copy the class directory to the new location\n",
    "            dest_class_path = os.path.join(output_dir, class_name)\n",
    "            shutil.copytree(class_path, dest_class_path)\n",
    "            print(f\"Class '{class_name}' retained with {len(image_files)} images.\")\n",
    "        else:\n",
    "            print(f\"Class '{class_name}' removed with only {len(image_files)} images.\")\n",
    "\n",
    "print(\"Filtered training data saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ba4f76-c0bb-49cf-ab4a-ef3f161cc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 8482\n",
      "Validation samples: 943\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load training data\n",
    "train_data_path = '/Users/anishvirkhare/Downloads/8_classes_train' \n",
    "train_data = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_dataset, val_dataset = random_split(train_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optional: Check dataset sizes\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c7bb74-1860-46c2-9816-00951bb44189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vision Transformer model with 23 classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model('vit_base_patch16_224', pretrained=True, num_classes=23).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df276607-48f8-440e-a4c3-80f602001135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d21aee-9965-47e7-8351-7b7e4ca19a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training: 100%|██████████████████| 266/266 [56:15<00:00, 12.69s/it]\n",
      "Epoch 1/10 - Validation: 100%|██████████████████| 30/30 [02:21<00:00,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Summary:\n",
      "  Training Loss: 2.0283, Training Accuracy: 22.51%\n",
      "  Validation Loss: 1.8985, Validation Accuracy: 30.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training: 100%|██████████████████| 266/266 [56:27<00:00, 12.73s/it]\n",
      "Epoch 2/10 - Validation: 100%|██████████████████| 30/30 [01:26<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Summary:\n",
      "  Training Loss: 1.7976, Training Accuracy: 32.06%\n",
      "  Validation Loss: 1.9898, Validation Accuracy: 28.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training: 100%|██████████████████| 266/266 [51:53<00:00, 11.70s/it]\n",
      "Epoch 3/10 - Validation: 100%|██████████████████| 30/30 [01:37<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Summary:\n",
      "  Training Loss: 1.7155, Training Accuracy: 36.49%\n",
      "  Validation Loss: 1.6998, Validation Accuracy: 36.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Training: 100%|██████████████████| 266/266 [52:35<00:00, 11.86s/it]\n",
      "Epoch 4/10 - Validation: 100%|██████████████████| 30/30 [01:41<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Summary:\n",
      "  Training Loss: 1.5718, Training Accuracy: 42.67%\n",
      "  Validation Loss: 1.7343, Validation Accuracy: 37.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Training: 100%|██████████████████| 266/266 [50:48<00:00, 11.46s/it]\n",
      "Epoch 5/10 - Validation: 100%|██████████████████| 30/30 [01:26<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Summary:\n",
      "  Training Loss: 1.5533, Training Accuracy: 42.71%\n",
      "  Validation Loss: 1.6756, Validation Accuracy: 42.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Training: 100%|██████████████████| 266/266 [51:41<00:00, 11.66s/it]\n",
      "Epoch 6/10 - Validation: 100%|██████████████████| 30/30 [01:23<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 Summary:\n",
      "  Training Loss: 1.4343, Training Accuracy: 47.94%\n",
      "  Validation Loss: 1.8902, Validation Accuracy: 36.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Training: 100%|██████████████████| 266/266 [52:25<00:00, 11.83s/it]\n",
      "Epoch 7/10 - Validation: 100%|██████████████████| 30/30 [01:29<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 Summary:\n",
      "  Training Loss: 1.3190, Training Accuracy: 51.98%\n",
      "  Validation Loss: 1.7276, Validation Accuracy: 40.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Training: 100%|██████████████████| 266/266 [49:04<00:00, 11.07s/it]\n",
      "Epoch 8/10 - Validation: 100%|██████████████████| 30/30 [01:28<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 Summary:\n",
      "  Training Loss: 1.2478, Training Accuracy: 54.75%\n",
      "  Validation Loss: 1.8452, Validation Accuracy: 39.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Training: 100%|██████████████████| 266/266 [47:31<00:00, 10.72s/it]\n",
      "Epoch 9/10 - Validation: 100%|██████████████████| 30/30 [01:22<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 Summary:\n",
      "  Training Loss: 1.1507, Training Accuracy: 58.21%\n",
      "  Validation Loss: 1.6159, Validation Accuracy: 43.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Training: 100%|█████████████████| 266/266 [47:33<00:00, 10.73s/it]\n",
      "Epoch 10/10 - Validation: 100%|█████████████████| 30/30 [01:28<00:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 Summary:\n",
      "  Training Loss: 1.0508, Training Accuracy: 62.54%\n",
      "  Validation Loss: 1.5618, Validation Accuracy: 46.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10   # Number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Training\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track training loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Track validation accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b49408-6d24-48e3-807f-6e20b137e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/Users/anishvirkhare/Downloads/final_ViT(8 classes).pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a4a50d-be51-4747-b0e0-3d66fce593e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/lhc7fyqd4x17cs8p6p_jsyrh0000gn/T/ipykernel_53892/225716031.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/Users/anishvirkhare/Downloads/final_ViT(8 classes).pth\",map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"/Users/anishvirkhare/Downloads/final_ViT(8 classes).pth\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "731e251c-9a8a-4ae7-8bb3-fe1f801786a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cad7da9-50a7-4d61-80e2-cf2c677a7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 2462\n"
     ]
    }
   ],
   "source": [
    "# Define the same transformations used during training\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize using ImageNet stats\n",
    "])\n",
    "\n",
    "# Load the test data\n",
    "test_data_path = \"/Users/anishvirkhare/Downloads/8_classes_test\"  # Replace with your test data path\n",
    "test_data = datasets.ImageFolder(root=test_data_path, transform=transform)\n",
    "\n",
    "# Create DataLoader for the test data\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Optional: Check test dataset size\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3128020-c7d4-45b1-8ee2-16f0a1696ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████| 77/77 [02:56<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "  Test Loss: 1.5462\n",
      "  Test Accuracy: 47.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "# Use the same loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "# Calculate metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Summary:\")\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1640dd2a-44b6-4f90-b1be-8e99bf81afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_t/lhc7fyqd4x17cs8p6p_jsyrh0000gn/T/ipykernel_1796/225716031.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/Users/anishvirkhare/Downloads/final_ViT(8 classes).pth\",map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"/Users/anishvirkhare/Downloads/final_ViT(8 classes).pth\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a9f11b-2cec-44d5-bcd0-ee373049016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████| 126/126 [04:58<00:00,  2.37s/it]\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary:\n",
      "  Test Loss: 8.9929\n",
      "  Test Accuracy: 9.67%\n",
      "\n",
      "Classification Report:\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                           Acne and Rosacea Photos       0.64      0.46      0.54       312\n",
      "Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions       0.29      0.51      0.37       288\n",
      "                                          Atopic Dermatitis Photos       0.07      0.31      0.12       123\n",
      "                                            Bullous Disease Photos       0.01      0.02      0.01       113\n",
      "                Cellulitis Impetigo and other Bacterial Infections       0.02      0.19      0.03        73\n",
      "                                                     Eczema Photos       0.02      0.08      0.04       309\n",
      "                                      Exanthems and Drug Eruptions       0.05      0.18      0.08       101\n",
      "                 Hair Loss Photos Alopecia and other Hair Diseases       0.00      0.02      0.01        60\n",
      "                                  Herpes HPV and other STDs Photos       0.00      0.00      0.00       102\n",
      "                      Light Diseases and Disorders of Pigmentation       0.00      0.00      0.00       143\n",
      "                        Lupus and other Connective Tissue diseases       0.00      0.00      0.00       105\n",
      "                               Melanoma Skin Cancer Nevi and Moles       0.00      0.00      0.00       116\n",
      "                                Nail Fungus and other Nail Disease       0.00      0.00      0.00       261\n",
      "                    Poison Ivy Photos and other Contact Dermatitis       0.00      0.00      0.00        65\n",
      "             Psoriasis pictures Lichen Planus and related diseases       0.00      0.00      0.00       352\n",
      "             Scabies Lyme Disease and other Infestations and Bites       0.00      0.00      0.00       108\n",
      "                      Seborrheic Keratoses and other Benign Tumors       0.00      0.00      0.00       343\n",
      "                                                  Systemic Disease       0.00      0.00      0.00       152\n",
      "            Tinea Ringworm Candidiasis and other Fungal Infections       0.00      0.00      0.00       325\n",
      "                                                   Urticaria Hives       0.00      0.00      0.00        53\n",
      "                                                   Vascular Tumors       0.00      0.00      0.00       121\n",
      "                                                 Vasculitis Photos       0.00      0.00      0.00       105\n",
      "                        Warts Molluscum and other Viral Infections       0.00      0.00      0.00       272\n",
      "\n",
      "                                                          accuracy                           0.10      4002\n",
      "                                                         macro avg       0.05      0.08      0.05      4002\n",
      "                                                      weighted avg       0.08      0.10      0.08      4002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/homebrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize metrics\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Use the same loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Track accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "\n",
    "        # Collect all labels and predictions for classification report\n",
    "        all_labels.extend(labels.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Summary:\")\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=test_loader.dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b941c95-2197-4917-88fb-745ad2c0f33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
